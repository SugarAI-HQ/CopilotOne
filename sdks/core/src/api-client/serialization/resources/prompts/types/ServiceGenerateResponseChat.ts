/**
 * This file was auto-generated by Fern from our API Definition.
 */

import * as serializers from "../../..";
import * as SugarAiApi from "../../../../api";
import * as core from "../../../../core";

export const ServiceGenerateResponseChat: core.serialization.ObjectSchema<
  serializers.ServiceGenerateResponseChat.Raw,
  SugarAiApi.ServiceGenerateResponseChat
> = core.serialization.object({
  id: core.serialization.string(),
  environment: core.serialization.lazy(
    async () =>
      (await import("../../..")).ServiceGenerateResponseChatEnvironment,
  ),
  version: core.serialization.string(),
  prompt: core.serialization.string(),
  completion: core.serialization.string().optional(),
  latency: core.serialization.number(),
  promptTokens: core.serialization.property(
    "prompt_tokens",
    core.serialization.number(),
  ),
  completionTokens: core.serialization.property(
    "completion_tokens",
    core.serialization.number(),
  ),
  llmResponse: core.serialization
    .lazy(
      async () =>
        (await import("../../..")).ServiceGenerateResponseChatLlmResponse,
    )
    .optional(),
  promptVariables: core.serialization
    .lazy(
      async () =>
        (await import("../../..")).ServiceGenerateResponseChatPromptVariables,
    )
    .optional(),
  totalTokens: core.serialization.property(
    "total_tokens",
    core.serialization.number(),
  ),
  labelledState: core.serialization.lazy(
    async () =>
      (await import("../../..")).ServiceGenerateResponseChatLabelledState,
  ),
  llmProvider: core.serialization.string(),
  llmModel: core.serialization.string(),
  llmModelType: core.serialization.lazy(
    async () =>
      (await import("../../..")).ServiceGenerateResponseChatLlmModelType,
  ),
  chat: core.serialization
    .lazy(
      async () => (await import("../../..")).ServiceGenerateResponseChatChat,
    )
    .optional(),
  createdAt: core.serialization.date(),
  updatedAt: core.serialization.date(),
  stats: core.serialization
    .lazy(
      async () => (await import("../../..")).ServiceGenerateResponseChatStats,
    )
    .optional(),
});

export declare namespace ServiceGenerateResponseChat {
  interface Raw {
    id: string;
    environment: serializers.ServiceGenerateResponseChatEnvironment.Raw;
    version: string;
    prompt: string;
    completion?: string | null;
    latency: number;
    prompt_tokens: number;
    completion_tokens: number;
    llmResponse?: serializers.ServiceGenerateResponseChatLlmResponse.Raw | null;
    promptVariables?: serializers.ServiceGenerateResponseChatPromptVariables.Raw | null;
    total_tokens: number;
    labelledState: serializers.ServiceGenerateResponseChatLabelledState.Raw;
    llmProvider: string;
    llmModel: string;
    llmModelType: serializers.ServiceGenerateResponseChatLlmModelType.Raw;
    chat?: serializers.ServiceGenerateResponseChatChat.Raw | null;
    createdAt: string;
    updatedAt: string;
    stats?: serializers.ServiceGenerateResponseChatStats.Raw | null;
  }
}
