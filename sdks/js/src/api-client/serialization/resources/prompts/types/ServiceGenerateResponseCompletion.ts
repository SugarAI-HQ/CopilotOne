/**
 * This file was auto-generated by Fern from our API Definition.
 */

import * as serializers from "../../..";
import * as SugarAiApi from "../../../../api";
import * as core from "../../../../core";

export const ServiceGenerateResponseCompletion: core.serialization.ObjectSchema<
  serializers.ServiceGenerateResponseCompletion.Raw,
  SugarAiApi.ServiceGenerateResponseCompletion
> = core.serialization.object({
  id: core.serialization.string(),
  environment: core.serialization.lazy(
    async () =>
      (await import("../../..")).ServiceGenerateResponseCompletionEnvironment,
  ),
  version: core.serialization.string(),
  prompt: core.serialization.string(),
  completion: core.serialization.string().optional(),
  latency: core.serialization.number(),
  promptTokens: core.serialization.property(
    "prompt_tokens",
    core.serialization.number(),
  ),
  completionTokens: core.serialization.property(
    "completion_tokens",
    core.serialization.number(),
  ),
  llmResponse: core.serialization
    .lazy(
      async () =>
        (await import("../../..")).ServiceGenerateResponseCompletionLlmResponse,
    )
    .optional(),
  promptVariables: core.serialization
    .lazy(
      async () =>
        (await import("../../.."))
          .ServiceGenerateResponseCompletionPromptVariables,
    )
    .optional(),
  totalTokens: core.serialization.property(
    "total_tokens",
    core.serialization.number(),
  ),
  labelledState: core.serialization.lazy(
    async () =>
      (await import("../../..")).ServiceGenerateResponseCompletionLabelledState,
  ),
  llmProvider: core.serialization.string(),
  llmModel: core.serialization.string(),
  llmModelType: core.serialization.lazy(
    async () =>
      (await import("../../..")).ServiceGenerateResponseCompletionLlmModelType,
  ),
  createdAt: core.serialization.date(),
  updatedAt: core.serialization.date(),
});

export declare namespace ServiceGenerateResponseCompletion {
  interface Raw {
    id: string;
    environment: serializers.ServiceGenerateResponseCompletionEnvironment.Raw;
    version: string;
    prompt: string;
    completion?: string | null;
    latency: number;
    prompt_tokens: number;
    completion_tokens: number;
    llmResponse?: serializers.ServiceGenerateResponseCompletionLlmResponse.Raw | null;
    promptVariables?: serializers.ServiceGenerateResponseCompletionPromptVariables.Raw | null;
    total_tokens: number;
    labelledState: serializers.ServiceGenerateResponseCompletionLabelledState.Raw;
    llmProvider: string;
    llmModel: string;
    llmModelType: serializers.ServiceGenerateResponseCompletionLlmModelType.Raw;
    createdAt: string;
    updatedAt: string;
  }
}
